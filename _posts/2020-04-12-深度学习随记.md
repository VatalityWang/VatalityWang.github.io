---
layout: post
title: 深度学习随记
categories: 学习
description: 感悟 随记
keywords: 历史 
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>





# code reading

## [infogan](https://github1s.com/eriklindernoren/PyTorch-GAN/blob/HEAD/implementations/infogan/infogan.py)

* 生成器

  * 输入高斯噪声，人为定义的标签，均匀分布向量；输出生成的图片。

* 判别器

  * 判别是生成的图片还是真实图片；输出该图片的标签，以及表征该图片语义的信息向量。

* 损失函数

  * 对抗损失：判断该图片是真假。

  ```python
   # Loss for real images
   real_pred, _, _ = discriminator(real_imgs)
   d_real_loss = adversarial_loss(real_pred, valid)
  
  # Loss for fake images
  fake_pred, _, _ = discriminator(gen_imgs.detach())
  d_fake_loss = adversarial_loss(fake_pred, fake)
  
  # Total discriminator loss
  d_loss = (d_real_loss + d_fake_loss) / 2
  ```

  

  * 分类损失：生成器预先输入类别和实际判别器输出类别之间的损失。

  * 信息损失: 生成器输入均匀分布向量和判别器输出信息向量之间的MSE损失。

  ```python
  # Sample labels
  sampled_labels = np.random.randint(0, opt.n_classes, batch_size)
  
  # Ground truth labels
  gt_labels = Variable(LongTensor(sampled_labels), requires_grad=False)
  
  # Sample noise, labels and code as generator input
  z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))
  label_input = to_categorical(sampled_labels, num_columns=opt.n_classes)
  code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt.code_dim))))
  
  gen_imgs = generator(z, label_input, code_input)
  _, pred_label, pred_code = discriminator(gen_imgs)
  
  info_loss = lambda_cat * categorical_loss(pred_label, gt_labels) + lambda_con * continuous_loss(
  pred_code, code_input
  )
  
  ```

* 测试

  * 输入：高斯噪声，连续变化的均匀分布向量，预设定标签。
  * 输出：标签对应类别的某一方面语义(字体的倾斜程度，粗细)连续变化的图片。

* 思考

  * 语义向量能否表示跨域个体特征的变化？
  * 生成器输入前一时期信号，判别器用来判断是前一时期还是后一时期，经过训练后输入前一时期的信号，输出后一时期变化后的信号，并且可以通过某一分布（均匀分布，连续）的参数来控制变化的大小。



# 深度学习几大框架历史



越是站在浪潮之巅，越要保持清晰的头脑!

[挑战 TensorFlow 与 PyTorch，3 月深度学习框架集中爆发](https://my.oschina.net/editorial-story/blog/3217606)

## 域适应

把具有不同分布的源域（source domain）何目标域（target domain）中的数据，映射到同一个特征空间，寻找某一种度量准则，使其在空间上的”距离“尽可能近；然后用源域（带标签）上训练好的分类器，就可以直接用于目标域数据的分类。

解决数据集之间的偏差。







InfoGan 学习到可解释性语义，能否通过此方法学习到个体细微特征。

# 信息熵

事件X=x的自信息(self-information)：

$I(x)=-log(P(x))$ log底数为e（自然对数）













